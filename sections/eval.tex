\subsection{Paradigm Support}

% good for dynamic
%% composite types are dynamic
Functional and dynamic languages

The groundwork is there, can primitives be used to make support?

\subsection{Performance Analysis}

When analyzing performance, results are relative to other implementations, as
this gives natural standpoint on what is considered good performance, and what
is considered bad. In our case it would be natural to compare the performance of
\thename{} to related work on stack based abstract machines. Here we have chosen
the previously mentioned Java Virtual Machine and Microsoft's Common Language
Runtime.

To make the the results of such a comparison as valuable as possible, it is
beneficial to focus on isolated parts, as there are many wheels turning in a
such a big piece of software. If one casts the net to wide the comparing, it can
be difficult to pin pointing the reasons for differences and similarities. Even
though JVM and CLR are both stack machines, they have characteristics which
distinguish them and makes to directly compare to each other, or \thename{} for
that matter.

We will therefore do out analysis through micro benchmarking. Opposed to
benchmarking in general, micro benchmarking focuses on isolated components or
specific functionality.

\subsubsection{Micro Benchmarking}

One way of creating benchmarks could be to write three programs, as similar as
possible, in programming languages which each has a front-end for the given
machine. For instance, we could write source code in Java to run on JVM and C\#
to run on CLR. The problem with this is that we cannot be sure of what code
the compiler produces. Two very similar programs in each of these two languages,
could in theory be compiled to be run very differently. To address this
challenge, we choose to disassemble the compiled to could to each of the
machines intermediate language, which is Java Bytecode and CIL. From the
disassembled version we can analyze which instructions each compiler chosen, and
in the case where they differ, we can manually alter them and assemble them back
to an executable.

When having two executables, one for JVM and one for CLR, and a \thename{} test
program, which we are satisfied with being as similar as possible, we are ready
to generate some results. We have chosen to generate the result through
automated shell scripts, timing the execution time for a set of parameters for
each machine. To make sure we did not get sporadic results caused by other
processes running on the host machine, we run the each benchmark of each machine
over several rounds. From this we can calculate the mean time for each
parameter.

For our micro benchmarks we have picked five different cases we wish to
test. These include stack operations, recursion, sub-routine invocation, boxing
and field operations on heap object. We will go each in turn, analyzing the
results.

% stack

To benchmark the stack we have made a simple program which essentially does a
lot of stack operations. A simple way to achieve this was to take a large number
and substitute one and stop ones it becomes zero. This requires many push and
pop operations, giving the stack a good workout. The result can be seen in the
seen below, in figure~\ref{fig:eval:benchmark:stack}.
\begin{figure}[H]
  \centering
  \scalebox{0.8}[0.7]{\input{figures/stack}}
  \caption{Mean running time of the stack workout}
\label{fig:eval:benchmark:stack}
\end{figure}

The graph's y-axis is the mean value of the time over all rounds. The x-axis
holds the start parameter on a logarithmic scale. We can clearly see JVM and CLR
performing better than \thename{}. This means TODO dark-side

% fibonacci

To benchmark recursion we have chosen to implement the classic Fibonacci
function, defined as $Fn = F_{n-1} + F_{n-2}$ with the seed values of $F_0 = 0$
and $F_1 = 1$. The result is shown below, in figure~\ref{fig:eval:benchmark:fib}.
\begin{figure}[H]
  \centering
  \scalebox{0.8}[0.7]{\input{figures/fib}}
  \caption{Mean running time of $F(n)$}
\label{fig:eval:benchmark:fib}
\end{figure}

The graph shows the $F_n$ on x-axis and mean mean time on the y-axis. When
analyzing the results, \thename{} are much closer to the performance of JVM. CIL
are still betterrrr TODO

% invocation

Sub-routines
\begin{figure}[H]
  \centering
  \scalebox{0.8}[0.7]{\input{figures/invoc}}
  \caption{Mean running time of sub-routine workout}
\end{figure}

% boxing

Boxing

% heap objects

Heap objects (w/ fields)

\subsubsection{Comparison with Existing Systems}

``Indentical'' programs in JVM, CIL

Tables, graphs

\subsubsection{Memory Footprint}

\subsubsection{Optimization}

During the majority of the development we did not concern ourselves with
algorithmic optimization, but rather focused on designing suitable data
structures and writing maintainable and concise code that we would be able to
cope with the inevitable changes during the course of the project. As Rob Pike
states it in his first Rule of Programming: ``You can't tell where a program is
going to spend its time.''\cite{pike-rules}. When the machine had reached a
maturity level that assured us that no further major changes to the overarching
design were necessary we started to analyze the code at a much more detailed
level.

Memory leaks are extremely difficult to avoid when writing any non-trivial C
code. One has to meticulously track allocated memory chunks and free them at
just the right time and place to prevent invalid read and writes. The fact that
C largely does not care about what some collection of bytes represents and
allows virtually any casting does not make this easier. One-off errors are very
common when dealing with arrays and pointers which are indeed a central part of
the \thename{} implementation. Luckily there are tools available to aid in
profiling and pinpointing problematic parts of the code. The absolutely
brilliant instrumentation tool Valgrind\footnote{Valgrind:
  \url{http://valgrind.org}} has been an invaluable help in detecting memory
errors and have allowed us to fix, to our knowledge, all problematic leaks.

Valgrind consists of several different tools that analyzes different things such
as threads, stack operations, caches and memory. We have used the memory error
detector called Memcheck. When an executable is run through Valgrind it is able
to track allocations and the corresponding releases made during the entire
execution. If an allocation does not have exactly one corresponding free
operation it is either a leak or a double free (when the same memory region is
attempted to be release more than once).

The output from Memcheck includes a list of errors and a summary of the leaked
amount of memory. When we initially ran Valgrind with the fibonacci test program
(with $n=15$) the leak summary was this:

\begin{verbatim}
==32067== LEAK SUMMARY:
==32067==    definitely lost: 410,033 bytes in 13,187 blocks
==32067==    indirectly lost: 371,808 bytes in 365 blocks
...
==32067== ERROR SUMMARY: 10 errors from 10 contexts (suppressed: 0 from 0)
\end{verbatim}

This says that there are 10 distinct errors causing a total leak of 400kB
memory, which is a very significant amount of memory for such a small and simple
program. Optimally there should be no bytes leaked at all. Valgrind also tells
you exactly where errors are introduced:

\begin{verbatim}
...
==32067== 27,608 bytes in 986 blocks are definitely lost in loss record 4 of 15
==32067==    at 0x4C29F90: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==32067==    by 0x4018F4: stack_create_element (stack.c:128)
...
\end{verbatim}

An allocation is being made in the function \code{stack\_create\_element} and is
not released when the program exits. In this case we were not properly releasing
temporary stack elements. Being aware of the issue we changed the way stack
elements were created and the result was a sum total of 40 bytes leaked all of
which are test data structures that are irrelevant for the actual performance
analysis.

The other form of profiling we did was by use of the equally wonderful tool
called gprof\footnote{GNU gprof:
  \url{https://sourceware.org/binutils/docs/gprof/}} (for GNU profiler). It
works in a somewhat different way than Valgrind: executables are compiled with a
special profiling flag that injects profiling code into the binary. When the
file is executed it generates data that is then run through gprof that performs
the analysis. The output is a detailed overview of the CPU time spent in each
function. It is important to avoid profiling optimized binaries because some
crucial information can be lost during when the compiler performs various code
transformations. For example, if the compiler inlines a function into another,
the former will appear to be using the resources that are actually spent by the
inlined function.

As an example of how we used gprof to pinpoint bottlenecks, an initial analysis
showed that 9\% of the total running time was spent in the function
\code{bytes2int} which converts a sequence of bytes into an integral C value. It
is a frequent operation but we found it is suspicious to be using almost one
tenth of the total CPU resources. It was indeed not optimally
implemented; Listing~\ref{lst:eval:gprof-pre} and~\ref{lst:eval:gprof-post}
shows the specific code before and after optimization.

\begin{lstlisting}[language={[ANSI]C},%
  caption={The function \code{bytes2int} before optimization.},%
  label={lst:eval:gprof-pre}]
int64_t value = 0;
int i = size;
while (i) {
    value += ((int64_t) bytes[i - 1]) << (8 * (size - i));
    i--;
}
return value;
\end{lstlisting}

\begin{lstlisting}[language={[ANSI]C},%
  caption={The function \code{bytes2int} after optimization},%
  label={lst:eval:gprof-post}]
switch (size) {
case 1: return bytes[0];
case 2: return bytes[1] | bytes[0] << 8;
case 4: return __builtin_bswap32(*(int32_t*)bytes);
case 8: return __builtin_bswap64(*(int64_t*)bytes);
default: return 0;
}
\end{lstlisting}

The optimized version of the function clocked in at the much better 1.4\% CPU
time. Using this technique of measuring, pinpointing, patching and measuring
again we managed to more than triple the speed of some test programs! Currently
the bottleneck of the machine seems to lie somewhere in the stack
implementation, as shown in the following gprof output:

\begin{verbatim}
  %   cumulative   self              self     total
 time   seconds   seconds    calls  ms/call  ms/call  name
 20.24      0.52     0.52 35002981     0.00     0.00  am_exec_instr
  8.17      0.73     0.21 24232831     0.00     0.00  stack_segment_push_element
  6.62      0.90     0.17 35002981     0.00     0.00  am_read_prefixes
  5.64      1.05     0.15 24232831     0.00     0.00  stack_push
  5.06      1.18     0.13 17501491     0.00     0.00  stack_peek
...
\end{verbatim}

Generally it is difficult to determine whether there is a problem with a part of
the code or if it is just a frequent operation in the specific
program. Profiling multiple test programs that utilize different features can
reveal common bottlenecks which should be analyzed further.

Whether there are more fundamental architectural design issues that limits
performance is difficult to say. We have not encountered major issues with how
the modules interoperate.

\subsection{Code Analysis}
\subsubsection{Code size (SLOC)}
\subsubsection{Architectural analysis}
\subsubsection{Readability, maintainability}
\subsubsection{Tests, coverage}
\subsubsection{Configurability (macros)}

\subsection{Documentation}

Very brief doxy description

% Usage evaluation
%% Have the problems been solved

% Assembly generation
%% Something about generating code for the machine

% Performance
%% Function calls
%% Analysis

% Future development
%% What's missing
%% What could be different
%% What would next steps be

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../report"
%%% End:
