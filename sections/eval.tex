There are many aspects to evaluation of a software project. In this section we
cover the most important factors. First we asses whether the overall goal of the
project has been reached, whether it solves the problems that we have
addressed. Disregarding the intention of the project we then look at how it
performs compared to other abstract machine implementation, namely JVM and
CIL. We then look at the architectural design in retrospect and determines
whether there were any fundamental design flaws that we would or should have
differently. We then go into the technical details of the system and describe
the process by which we have performed performance measurements and
corresponding optimization.

\subsection{Paradigm Support}

The initial goal of the project was to implement an abstract machine more
suitable for modern language paradigms than the popular contemporary
solutions. Contained in this is the obvious challenge of actually implementing
an abstract machine from scratch with all the bells and whistles required.

To begin with we would like to stress the fact that we did not design the
specification for \thename{}. We have been in close collaboration with our
advisor Sven Karlsson who have done the all the concrete writing of it.

A fundamental aspect of the design of \thename{} is that it exposes primitives
that a compiler can use to construct a run-time system suitable for the language
at hand. We believe this has been accomplished to a large extent.

The concept of an \code{AnyType} capable of expressing arbitrary values can make
code generation for dynamic languages much easier because variables do not have
to declare their type and because the type can change during run-time. Composite
types are similarly dynamic in the sense that members can be added and removed
on demand which is common for many dynamic languages. It also provides easy
dynamic dispatch because \thename{} automatically can look up a sub-routine
member by name.

One of the most powerful features of \thename{} is the fact that at its core it
is very simple and flexible which makes it capable of expressing many language
features by combining those building blocks in various ways. There are few
restrictions as to how the primitives can be used.

As for functional language support, the way sub-routines work could be a
significant improvement. In JVM and CIL a sub-routine \emph{must} be attached to
an object, but \thename{} allows sub-routines to exists by themselves and can be
loaded and executed from anywhere. The execution context can be controlled by
managing the use of scopes, something that the compiler is free to do how it
sees fit.

The current implementation is not mature enough to be a sensible target for code
generation, but it does serve as a proof-of-concept that the fundamental ideas
work and could be made production ready. A serious effort could result in a
significant improvement to what is otherwise available.

\subsection{Performance Analysis}

When analyzing performance, results are relative to other implementations, as
this gives natural standpoint on what is considered good performance, and what
is considered bad. In our case it would be natural to compare the performance of
\thename{} to related work on stack based abstract machines. Here we have chosen
the previously mentioned Java Virtual Machine and Microsoft's Common Language
Runtime.

To make the the results of such a comparison as valuable as possible, it is
beneficial to focus on isolated parts, as there are many wheels turning in a
such a big piece of software. If one casts the net to wide the comparing, it can
be difficult to pin pointing the reasons for differences and similarities. Even
though JVM and CLR are both stack machines, they have characteristics which
distinguish them and makes to directly compare to each other, or \thename{} for
that matter.

We will therefore do out analysis through micro benchmarking. Opposed to
benchmarking in general, micro benchmarking focuses on isolated components or
specific functionality.

\subsubsection{Micro Benchmarking}

One way of creating benchmarks could be to write three programs, as similar as
possible, in programming languages which each has a front-end for the given
machine. For instance, we could write source code in Java to run on JVM and C\#
to run on CLR. The problem with this is that we cannot be sure of what code
the compiler produces. Two very similar programs in each of these two languages,
could in theory be compiled to be run very differently. To address this
challenge, we choose to disassemble the compiled to could to each of the
machines intermediate language, which is Java Bytecode and CIL. From the
disassembled version we can analyze which instructions each compiler chosen, and
in the case where they differ, we can manually alter them and assemble them back
to an executable.

When having two executables, one for JVM and one for CLR, and a \thename{} test
program, which we are satisfied with being as similar as possible, we are ready
to generate some results. We have chosen to generate the result through
automated shell scripts, timing the execution time for a set of parameters for
each machine. To make sure we did not get sporadic results caused by other
processes running on the host machine, we run the each benchmark of each machine
over several rounds. From this we can calculate the mean time for each
parameter.

For our micro benchmarks we have picked four different cases we wish to
test. These include stack operations, recursion, sub-routine invocation and
field operations on heap object. We will go each in turn, analyzing the results.

% stack

To benchmark the stack we have made a simple program which essentially does a
lot of stack operations. A simple way to achieve this was to take a large number
and substitute until it becomes zero. More specifically, we start by using
\instr{pushLiteral} with the \instr{large} prefix to get a large number on the
stack. We there after push the literal one, push the large number again with
\instr{pushElement} and perform substitution with \instr{sub}. To store the
result, we duplicate it and use \instr{storeElement} to overwrite the initial
number. Lastly we check if the result is zero, in which case it is not, we
branch back to the top, starting over. This induces 10 stack operation per
iteration, plus the initial one, which equals $10n + 1$, where
$n = 1\ ...\ (2^{31} -1)$, giving the complexity of $O(n)$. CIL does not have an
\instr{storeElement} instruction, but achieves the same effect by storing the
value in a local variable and then loading it again, done with {\tt stloc} and
{\tt ldloc}.

The result can be seen in the seen below, in
figure~\ref{fig:eval:benchmark:stack}.
\begin{figure}[H]
  \centering
  \scalebox{0.8}[0.6]{\input{figures/stack}}
  \caption{Mean running time of the stack workout.}
\label{fig:eval:benchmark:stack}
\end{figure}

The graph's y-axis is the mean value of time spent during all rounds and the
x-axis represents the iterations of the stack routine performed. Due to the
x-axis being logarithmic, it may immediately seem that the results are fairly
close, but there is actually a vast difference in performance. Both JVM and CLR
are about two orders of magnitude faster than \thename{} for all
$n>10^6$. Interestingly however, \thename{} is faster up until that point, which
suggests that JVM and CLR are either using special techniques for extremely
stack heavy programs or that they spend a significant amount of time to start up
the machinery. We are inclined to believe that the latter is the cause because
it is true for the remaining test cases as well.

% fibonacci

To benchmark recursion we have chosen to implement the classic Fibonacci
function, defined as $Fn = F_{n-1} + F_{n-2}$ with the seed values of $F_0 = 0$
and $F_1 = 1$. The function makes two recursive calls to itself per value of
$n$, creating a recursion tree of depth $n$, with the leafs being seed
values. Intuitively, we know this creates the exponential running of $O(2^n)$,
which could be proved by induction. We will not include the proof here, as it is
out the scope of this discussion. The benchmark result is shown below, in
figure~\ref{fig:eval:benchmark:fib}.

\begin{figure}[H]
  \centering
  \scalebox{0.8}[0.6]{\input{figures/fib}}
  \caption{Mean running time of $F(n)$}
\label{fig:eval:benchmark:fib}
\end{figure}

The graph shows the $F_n$ on x-axis and mean mean time on the y-axis. Here
\thename{} is almost on par with JVM, both of which lack behind CLR. That is an
indication of efficient sub-routine and stack memory mechanisms in
\thename{}. It is important to note that these are all na\"ive implementations
with no optimizations, such as tail-recursion, which would have a significant
positive impact on the running time.

% invocation

The sub-routing benchmark is implemented in a similar fashion as the stack
workout program, only the calculation is performed in a sub-routine which is
invoked each iteration. All the stack operations done in the sub-routine in
executed in constant time, which given the resulting running time of
$T(n) = O(n) \cdot O(k) = O(n)$, where $k$ is constant the number of stack
operations. For invoking the sub-routine we use the \instr{invoke} instructions,
while CLR uses {\tt call} and JVM uses {\tt invokestatic}. The result of the
benchmark can be seen below, in figure~\ref{fig:eval:benchmark:invoc}.

\begin{figure}[H]
  \centering
  \scalebox{0.8}[0.6]{\input{figures/invoc}}
  \caption{Mean running time of sub-routine workout}
\label{fig:eval:benchmark:invoc}
\end{figure}

Again \thename{} is \emph{relatively} close to JVM. It is interesting that the
graph for both JVM and \thename{} is jagged for high values of $n$ while CLRs
performance shows a more steady curve. Exactly what this is due to is difficult
to say.

% heap objects

The last benchmark focuses on field operations on heap objects, namely the
\instr{pushFieldHeapObject} and \instr{popFieldHeapObject} instructions. It does
this by initially creating an instance of a simple heap object with a single
integer field. It then does $n$ iterations of pushing it's field to the stack,
incrementing it and popping it back to the heap object. The running time becomes
the same as the sub-routine benchmark; $T(n) = O(n) \cdot O(k) = O(n)$, where
$k$ is the constant number of stack operations. The result of the
benchmark can be seen below, in figure~\ref{fig:eval:benchmark:heap}.
\begin{figure}[H]
  \centering
  \scalebox{0.8}[0.6]{\input{figures/heap}}
  \caption{Mean running time of heap workout}
\label{fig:eval:benchmark:heap}
\end{figure}

The graph shows the mean-time of all round on the y-axis and start parameter $n$
on the logarithmic x-axis. We see the same pattern as previous benchmarks,
without many irregularities. \thename{} is still being outperformed by JVM and
CLR, but continues to be have a better startup time.


\subsubsection{Memory Footprint}

\subsubsection{Optimization}

During the majority of the development we did not concern ourselves with
algorithmic optimization, but rather focused on designing suitable data
structures and writing maintainable and concise code that we would be able to
cope with the inevitable changes during the course of the project. As Rob Pike
states it in his first Rule of Programming: ``You can't tell where a program is
going to spend its time.''\cite{pike-rules}. When the machine had reached a
maturity level that assured us that no further major changes to the overarching
design were necessary we started to analyze the code at a much more detailed
level.

Memory leaks are extremely difficult to avoid when writing any non-trivial C
code. One has to meticulously track allocated memory chunks and free them at
just the right time and place to prevent invalid read and writes. The fact that
C largely does not care about what some collection of bytes represents and
allows virtually any casting does not make this easier. One-off errors are very
common when dealing with arrays and pointers which are indeed a central part of
the \thename{} implementation. Luckily there are tools available to aid in
profiling and pinpointing problematic parts of the code. The absolutely
brilliant instrumentation tool Valgrind\footnote{Valgrind:
  \url{http://valgrind.org}} has been an invaluable help in detecting memory
errors and have allowed us to fix, to our knowledge, all problematic leaks.

Valgrind consists of several different tools that analyzes different things such
as threads, stack operations, caches and memory. We have used the memory error
detector called Memcheck. When an executable is run through Valgrind it is able
to track allocations and the corresponding releases made during the entire
execution. If an allocation does not have exactly one corresponding free
operation it is either a leak or a double free (when the same memory region is
attempted to be release more than once).

The output from Memcheck includes a list of errors and a summary of the leaked
amount of memory. When we initially ran Valgrind with the fibonacci test program
(with $n=15$) the leak summary was this:

\begin{verbatim}
==32067== LEAK SUMMARY:
==32067==    definitely lost: 410,033 bytes in 13,187 blocks
==32067==    indirectly lost: 371,808 bytes in 365 blocks
...
==32067== ERROR SUMMARY: 10 errors from 10 contexts (suppressed: 0 from 0)
\end{verbatim}

This says that there are 10 distinct errors causing a total leak of 400kB
memory, which is a very significant amount of memory for such a small and simple
program. Optimally there should be no bytes leaked at all. Valgrind also tells
you exactly where errors are introduced:

\begin{verbatim}
...
==32067== 27,608 bytes in 986 blocks are definitely lost in loss record 4 of 15
==32067==    at 0x4C29F90: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==32067==    by 0x4018F4: stack_create_element (stack.c:128)
...
\end{verbatim}

An allocation is being made in the function \code{stack\_create\_element} and is
not released when the program exits. In this case we were not properly releasing
temporary stack elements. Being aware of the issue we changed the way stack
elements were created and the result was a sum total of 40 bytes leaked all of
which are test data structures that are irrelevant for the actual performance
analysis.

The other form of profiling we did was by use of the equally wonderful tool
called gprof\footnote{GNU gprof:
  \url{https://sourceware.org/binutils/docs/gprof/}} (for GNU profiler). It
works in a somewhat different way than Valgrind: executables are compiled with a
special profiling flag that injects profiling code into the binary. When the
file is executed it generates data that is then run through gprof that performs
the analysis. The output is a detailed overview of the CPU time spent in each
function. It is important to avoid profiling optimized binaries because some
crucial information can be lost during when the compiler performs various code
transformations. For example, if the compiler inlines a function into another,
the former will appear to be using the resources that are actually spent by the
inlined function.

As an example of how we used gprof to pinpoint bottlenecks, an initial analysis
showed that 9\% of the total running time was spent in the function
\code{bytes2int} which converts a sequence of bytes into an integral C value. It
is a frequent operation but we found it is suspicious to be using almost one
tenth of the total CPU resources. It was indeed not optimally
implemented; Listing~\ref{lst:eval:gprof-pre} and~\ref{lst:eval:gprof-post}
shows the specific code before and after optimization.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language={[ANSI]C},%
  caption={The function \code{bytes2int} before optimization.},%
  label={lst:eval:gprof-pre}]
int64_t value = 0;
int i = size;
while (i) {
    value += ((int64_t) bytes[i - 1]) << (8 * (size - i));
    i--;
}
return value;
\end{lstlisting}

\begin{lstlisting}[language={[ANSI]C},%
  caption={The function \code{bytes2int} after optimization},%
  label={lst:eval:gprof-post}]
switch (size) {
case 1: return bytes[0];
case 2: return bytes[1] | bytes[0] << 8;
case 4: return __builtin_bswap32(*(int32_t*)bytes);
case 8: return __builtin_bswap64(*(int64_t*)bytes);
default: return 0;
}
\end{lstlisting}
\end{minipage}

The optimized version of the function clocked in at the much better 1.4\% CPU
time. Using this technique of measuring, pinpointing, patching and measuring
again we managed to more than triple the speed of some test programs! Currently
the bottleneck of the machine seems to lie somewhere in the stack
implementation, as shown in the following gprof output:

\begin{verbatim}
  %   cumulative   self              self     total
 time   seconds   seconds    calls  ms/call  ms/call  name
 20.24      0.52     0.52 35002981     0.00     0.00  am_exec_instr
  8.17      0.73     0.21 24232831     0.00     0.00  stack_segment_push_element
  6.62      0.90     0.17 35002981     0.00     0.00  am_read_prefixes
  5.64      1.05     0.15 24232831     0.00     0.00  stack_push
  5.06      1.18     0.13 17501491     0.00     0.00  stack_peek
...
\end{verbatim}

Generally it is difficult to determine whether there is a problem with a part of
the code or if it is just a frequent operation in the specific
program. Profiling multiple test programs that utilize different features can
reveal common bottlenecks which should be analyzed further.

Whether there are more fundamental architectural design issues that limits
performance is difficult to say. We have not encountered major issues with how
the modules interoperate.

% JIT

A common form of optimization in modern abstract machines is the process of
compiling a program run-time, rather than prior to execution. This technique is
called just-in-time compilation, or JIT. This allows sections of code to be
translated to machine code which host machine can execute directly. This is done
right before the code is meant to be executed, hence the name just-in-time. The
translated machine code can then be cached and reused. Therefore, if a function
is called multiple times, the machine can save time by paying the overhead of
translating it to machine code, but then saving time every time the same code is
to be executed. A challenge with JITing is balancing the overhead of translating
to machine code versus the saved time during execution. This overhead of
just-in-time compilation is called the `startup time delay`.

In our earlier performance analysis, we have disabled all forms for such
optimizations to make the execution process of the different machines as similar
as possible. Below we can see an example of the heap workout benchmark being run
on JVM with and without JIT optimization. The result can be seen below, in
figure~\ref{fig:eval:benchmark:jit}.

\begin{figure}[H]
  \centering
  \scalebox{0.8}[0.6]{\input{figures/jit}}
  \caption{JIT comparing on JVM}
\label{fig:eval:benchmark:jit}
\end{figure}

We can see the huge benefit JIT compilation TODO. Even with small numbers, we
see that the statup time delay that occurs when using JIT is still outweighed
by the faster execution time.

\subsection{Code Analysis}

Another side of code evaluation is to view the source code from a macro
perspective, including trivial things such as counting source lines of code
(SLOC) and looking at the number of comments per line of
code. Figure~\ref{fig:eval:sloc} shows the division of code in the source
directory and in the tests. The amount of source code is well correlated with
the amount of work performed by each module. The test SLOC are not proportional
to the amount of code in the corresponding module because some algorithms
require much more extensive unit tests than others. For instance the type
conversion algorithms must cover almost all different combinations of types
which results in extensive tests.

\begin{figure}
  \centering
  \input{figures/sloc}
  \caption{SLOC in \thename{}}
\label{fig:eval:sloc}
\end{figure}

The overall architecture has proved itself to be a well suited design that
allowed us to modify functionality, even larger non-trivial changes, without
causing significant trouble or breaking other parts of the system. The division
of modules and their responsibilities was largely based on intuitive decisions
about what categories of functionality were needed. That said, there is room for
improvement in several areas of the system. The instruction execution module has
become large and to some extent unwieldy, and should optimally be split into
smaller sub-modules each implementing a specific family or type of
instructions. There are some repeating patterns of code that we have so far kept
DRY\footnote{``Don't Repeat Yourself''} by use of macros making it difficult to
maintain, reason about and debug and would be better off implemented by
combining smaller and more concise functions.

We have strived to keep the code readable and easy to digest and believe we have
succeeded in this. However, as we have optimized the code, some algorithms have
become more complex and difficult to grasp at first sight, but that is something
we find unavoidable to some extent.

\subsubsection{Tests, coverage}



\subsection{Documentation}

The code is fairly well documented using Doxygen, which has been a help when
reviewing and when continuing work on code written by the other. There are
approximately 14 lines of comments for each 100 SLOC which should provide a
decent amount of explanation.

Appendix~\ref{NEEDED} documents how to compile and run the machine on a Unix
system.

% Future development
%% What's missing
%% What could be different
%% What would next steps be

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../report"
%%% End:
