There are many aspects to evaluation of a software project. In this section we
cover the most important factors. First we asses whether the overall goal of the
project has been reached, whether it solves the problems that we have
addressed. Disregarding the intention of the project we then look at how it
performs compared to other abstract machine implementation, namely JVM and
CIL. We then look at the architectural design in retrospect and determines
whether there were any fundamental design flaws that we would or should have
differently. We then go into the technical details of the system and describe
the process by which we have performed performance measurements and
corresponding optimization.

\subsection{Paradigm Support}

The initial goal of the project was to implement an abstract machine more
suitable for modern language paradigms than the popular contemporary
solutions. Contained in this is the obvious challenge of actually implementing
an abstract machine from scratch with all the bells and whistles required.

To begin with we would like to stress the fact that we did not design the
specification for \thename{}. We have been in close collaboration with our
advisor Sven Karlsson who have done the all the concrete writing of it.

A fundamental aspect of the design of \thename{} is that it exposes primitives
that a compiler can use to construct a run-time system suitable for the language
at hand. We believe this has been accomplished to a large extent.

The concept of an \code{AnyType} capable of expressing arbitrary values can make
code generation for dynamic languages much easier because variables do not have
to declare their type and because the type can change during run-time. Composite
types are similarly dynamic in the sense that members can be added and removed
on demand which is common for many dynamic languages. It also provides easy
dynamic dispatch because \thename{} automatically can look up a sub-routine
member by name.

One of the most powerful features of \thename{} is the fact that at its core it
is very simple and flexible which makes it capable of expressing many language
features by combining the provided building blocks in various ways. There are
few restrictions as to how the primitives can be used.

As for functional language support, the way sub-routines work could be a
significant improvement. In JVM and CIL a sub-routine \emph{must} be attached to
an object, but \thename{} allows sub-routines to exists by themselves and can be
loaded and executed from anywhere facilitation first-class functions. The
execution context can be controlled by managing the use of scopes, something
that the compiler is free to do how it sees fit.

The current implementation is not mature enough to be a sensible target for code
generation, but it does serve as a proof-of-concept that the fundamental ideas
work and could be made production ready. A serious effort could result in a
significant improvement to what is otherwise available.

\subsection{Performance Analysis}
\label{sec:eval:performance}

When analyzing performance, results are relative to other implementations, as
this gives natural standpoint on what is considered good performance, and what
is considered bad. In our case it would be natural to compare the performance of
\thename{} to related work on stack based abstract machines. Here we have chosen
the previously mentioned Java Virtual Machine and Microsoft's Common Language
Runtime.

To make the the results of such a comparison as valuable as possible, it is
beneficial to focus on isolated functionality, as there are many wheels turning
in a such a big piece of software. If one casts the net too wide, it can be
difficult to interpret what results are indicating. Even though JVM and CLR are
both stack machines, they have characteristics which distinguish them and makes
to directly compare to each other, or \thename{} for that matter. %XXX wat@sidste sÃ¦tning

We will therefore perform analysis through so-called micro benchmarking, which
as opposed to benchmarking in general, focuses on isolated components or
specific functionality.

\subsubsection{Micro Benchmarking}

One way of creating benchmarks would be to write small programs, as similar as
possible, in programming languages which each has a front-end for the given
machine. For instance, we could write source code in Java to run on JVM and C\#
to run on CLR. The problem with this is that we cannot be sure of what code the
compiler produces. Two very similar programs in each of these two languages,
could likely be compiled to very different byte code. To address this issue, we
disassembled the compiled code to each of the machine's intermediate languages,
namely Java Bytecode and CIL. From the disassembled code we analyzed which
instructions each compiler emitted and in cases where they differed
substantially, we manually altered them and assemble them back to an executable.

Having two executables, one for JVM and one for CLR, and a \thename{} test
program, which we are satisfied with being as similar as possible, we start
measuring their performance. We ran the tests through automated shell scripts
that measured the execution time for a set of parameters for each machine. To
make sure we did not get sporadic results caused by other processes running on
the host machine, we ran each benchmark of each machine over several
rounds. From this we calculated the mean running time for each parameter on each
machine.

We have picked four different cases that we wish to measure, namely stack
operations, recursion, sub-routine invocation and field operations on heap
object. We will go through each in turn, analyzing the results.

% stack

To benchmark the stack we made a simple program which essentially just does a
lot of trivial stack operations. A simple way to achieve this was to take a
large number and decrement it until it becomes zero.

\begin{stackops}
  \op{large pushLiteral i32 n}{n}
  \op{pushLiteral i32 1}{n,\ 1}
  \op{pushElement 1}{n,\ 1,\ n}
  \op{sub}{n,\ (n-1)}
  \op{pushElement 0}{n,\ (n-1),\ (n-1)}
  \op{storeElement 2}{(n-1),\ (n-1)}
  \op{cmpZero}{(n-1),\ b}
  \op{brFalse -13}{(n-1)}
  \op{halt}{-}
\end{stackops}

This induces 9 stack operation per iteration, plus the initial one, which
equals $9n + 1$, where $n = 1\ ...\ (2^{31} -1)$, giving the complexity of
$O(n)$. CIL does not have an \instr{storeElement} instruction, but achieves the
same effect by storing the value in a local variable and then loading it again,
done with {\tt stloc} and {\tt ldloc} (store / load local variable).

The result can be seen in the seen below, in
Figure~\ref{fig:eval:benchmark:stack}.
\begin{figure}[H]
  \centering
  \scalebox{0.8}[0.6]{\input{figures/stack}}
  \caption{Mean running time of the stack workout.}
\label{fig:eval:benchmark:stack}
\end{figure}

The graph's y-axis is the mean value of time spent during all rounds and the
x-axis represents the iterations of the stack routine performed. Due to the
x-axis being logarithmic, it may immediately seem that the results are fairly
close, but there is actually a vast difference in performance. Both JVM and CLR
are about two orders of magnitude faster than \thename{} for all
$n>10^6$. Interestingly however, \thename{} is faster up until that point, which
suggests that JVM and CLR are either using special techniques for extremely
stack heavy programs or that they spend a significant amount of time to start up
the machinery. We are inclined to believe that the latter is the cause because
it is true for the remaining test cases as well.

% fibonacci

To benchmark recursion we implemented the classic Fibonacci function, defined
as:

\begin{equation*}
  fib(n) = \begin{cases}
    0                   & n = 0 \\
    1                   & n = 1 \\
    fib(n-1) + fib(n-2) & \text{otherwise}
  \end{cases}
\end{equation*}

The implementation is intentionally na\"ive, resulting in exponential running
time $O(2^n)$, which could be proved by induction. We will not include the proof
here, as it is irrelevant for this discussion. The benchmark result is shown
below, in Figure~\ref{fig:eval:benchmark:fib}.

\begin{figure}[H]
  \centering
  \scalebox{0.8}[0.6]{\input{figures/fib}}
  \caption{Mean running time of $fib(n)$}
\label{fig:eval:benchmark:fib}
\end{figure}

The graph shows the $fib(n)$ on x-axis and mean time on the y-axis. \thename{}
is almost on par with JVM, both of which lack behind CLR. That is an indication
of efficient sub-routine and stack memory mechanisms in \thename{}. It is
important to remember that these are all na\"ive implementations with no
optimizations such as tail-recursion or dynamic programming techniques, which
would have a significant impact on the running time.

% invocation

The sub-routing benchmark is implemented in a similar fashion as the stack
workout program, only the calculation is performed in a sub-routine which is
invoked each iteration. All the stack operations done in the sub-routine are
executed in constant time, which given the resulting running time of
$T(n) = O(n) \cdot O(k) = O(n)$, where $k$ is the constant number of stack
operations. For invoking the sub-routine we use the \instr{invoke} instructions,
while CLR uses {\tt call} and JVM uses {\tt invokestatic}. The result of the
benchmark can be seen below, in Figure~\ref{fig:eval:benchmark:invoc}.

\begin{figure}[H]
  \centering
  \scalebox{0.8}[0.6]{\input{figures/invoc}}
  \caption{Mean running time of sub-routine workout}
\label{fig:eval:benchmark:invoc}
\end{figure}

Again \thename{} is \emph{relatively} close to JVM. It is interesting that the
graph for both JVM and \thename{} is jagged for high values of $n$ while CLRs
performance shows a more steady curve. Exactly what this is due to is difficult
to say.

% heap objects

The last benchmark focuses on field operations on heap objects, namely the
\instr{pushFieldHeapObject} and \instr{popFieldHeapObject} instructions. It does
this by initially creating an instance of a simple heap object with a single
integer field. It then does $n$ iterations of pushing it's field onto the stack,
incrementing it and popping it back to the heap object. The running time becomes
the same as the sub-routine benchmark; $T(n) = O(n) \cdot O(k) = O(n)$, where
$k$ is the constant number of stack operations. The result of the benchmark can
be seen below, in Figure~\ref{fig:eval:benchmark:heap}.

\begin{figure}[H]
  \centering
  \scalebox{0.8}[0.6]{\input{figures/heap}}
  \caption{Mean running time of heap workout}
\label{fig:eval:benchmark:heap}
\end{figure}

The graph shows the mean-time of all round on the y-axis and start parameter $n$
on the logarithmic x-axis. We see the same pattern as previous benchmarks,
without many irregularities. \thename{} is still being outperformed by JVM and
CLR, but continues to be have a better startup time.

\subsubsection{Memory Footprint}

When analyzing the memory footprint of \thename{} we used the absolutely
brilliant instrumentation tool Valgrind\footnote{Valgrind:
  \url{http://valgrind.org}}. It takes the program which is to be run as
argument, and does so in a sand-boxed environment, allowing it to analyze the
program's resource consumption together with multitudes of other things. To
target the machine's memory consumption specifically, we utilize one of
Valgrind's many built-in tools, which is called Massif\footnote{Massif Heap
  Profiler: \url{http://valgrind.org/docs/manual/ms-manual.html}}. This is a
heap profiler, able of analyzing the amount of allocated memory of each function
in the given program.

We have made a heap profile of \thename{} running the Fibonacci recursion
benchmark, shown in the previous section. We have run Valgrind with the
following arguments, and the result can be seen in
Figure~\ref{fig:heap-profile}, generated with
Massif-Visualizer\footnote{Massif-Visualizer:
  \url{http://milianw.de/tag/massif-visualizer}}.

{\tt valgrind --tool=massif --time-unit=B bin/am}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.35]{figures/fig-mem}
  \caption{Heap profile of \thename{} running Fibonacci benchmark program}
\label{fig:heap-profile}
\end{figure}

The graph shows the memory heap size (in kB) on the y-axis and bytes allocated
and freed on the x-axis. Having the x-axis in bytes gives an alternative
representation of time, more beneficial when the actual running time is
irrelevant. Looking at the graph we see a pyramid shaped memory usage over the
duration of the program. The inner red fill of the pyramid is that used by the
the machine's stack, while the outer linings are information tables, threads
etc. This shows us that during start-up, memory is consumed by the information
tables and other core infrastructure, but following memory is solely consumed by
the stack. This makes good sense, as the Fibonacci benchmark does not mutate the
machine's state, with exception of the main thread's stack. The stack keeps
growing til half way into the program, and thereafter it declines at the same
pace. This pattern occurs because of how the Fibonacci program generates a very
deep recursion tree, as mentioned in the previous section. For each recursion,
two sub-routine invocations occur, resulting in two new activation frames being
pushed to the stack. When the the recursion tree finally ends in a leaf, the
value of $fib(n)$ is found, and the value is returned all the way up the
recursion tree, popping activation elements as it goes, until eventually being
returned to the original caller.

The result are indicating no unnecessary or unexpected use of memory.

\subsubsection{Optimization}

During the majority of the development we did not concern ourselves with
algorithmic optimization, but rather focused on designing suitable data
structures and writing maintainable and concise code that we would be able to
cope with the inevitable changes during the course of the project. As Rob Pike
states it in his first Rule of Programming: ``You can't tell where a program is
going to spend its time. Bottlenecks occur in surprising places, so don't try to
second guess and put in a speed hack until you've proven that's where the
bottleneck is.''\cite{pike-rules}. When the machine had reached a maturity level
that assured us that no further major changes to the overarching design were
necessary we started to analyze the code at a much more detailed level.

Memory leaks are extremely difficult to avoid when writing any non-trivial C
code. One has to meticulously track allocated memory chunks and free them at
just the right time and place to prevent invalid reads and writes. The fact that
C largely does not care about what some collection of bytes represents and
allows virtually any casting does not make this easier. One-off errors are
common when dealing with arrays and pointers which are indeed a central part of
the \thename{} implementation. Luckily there are tools available to aid in
profiling and pinpointing problematic parts of the code. Valgrind has been an
invaluable help for detecting memory errors and have allowed us to fix, to our
knowledge, all problematic leaks.

Valgrind consists of several different tools that analyze different things like
threads, stack operations, caches and memory. We have used the memory error
detector called Memcheck for finding leaks. When an executable is run through
Valgrind it is able to track allocations and the corresponding releases made
during the entire execution. If an allocation does not have exactly one
corresponding free operation it is either a leak or a double free (i.e.~a single
memory region attempted to be released more than once).

The output from Memcheck includes a list of errors and a summary of the leaked
amount of memory. When we initially ran Valgrind with the fibonacci test program
(with $n=15$) the leak summary was this:

\begin{verbatim}
==32067== LEAK SUMMARY:
==32067==    definitely lost: 410,033 bytes in 13,187 blocks
==32067==    indirectly lost: 371,808 bytes in 365 blocks
...
==32067== ERROR SUMMARY: 10 errors from 10 contexts (suppressed: 0 from 0)
\end{verbatim}

This says that there are 10 distinct errors causing a total leak of 400kB
memory, which is a very significant amount of memory for such a small and simple
program. Optimally there should be no bytes leaked at all. Valgrind also tells
you exactly where errors are introduced:

\begin{verbatim}
...
==32067== 27,608 bytes in 986 blocks are definitely lost in loss record 4 of 15
==32067==    at 0x4C29F90: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==32067==    by 0x4018F4: stack_create_element (stack.c:128)
...
\end{verbatim}

An allocation is being made in the function \code{stack\_create\_element} and is
not released when the program exits. In this case we were not properly releasing
temporary stack elements, and being aware of the issue we changed the way stack
elements are created resulting in a sum total of 40 bytes leaked, all of which
are test data structures that are irrelevant for the actual performance
analysis. It is important to run multiple types of test programs through
Valgrind, because each of them may expose errors in differents parts of the
code.

The other form of profiling we did was by use of the equally great tool called
gprof\footnote{GNU gprof: \url{https://sourceware.org/binutils/docs/gprof/}}
(for GNU profiler). It works in a somewhat different way than Valgrind:
executables are compiled with a special profiling flag that injects profiling
code into the binary. When the file is executed it generates data that is then
run through gprof that performs the analysis. The output is a detailed overview
of the CPU time spent in each function. It is important to avoid profiling
optimized binaries because some crucial information can be lost when the
compiler performs various code transformations. For example, if the compiler
inlines a function into another, the former will appear to be using the
resources that are actually spent by the inlined function.

As an example of how we used gprof to pinpoint bottlenecks, an initial analysis
showed that 9\% of the total running time was spent in the function
\code{bytes2int} which converts a sequence of bytes into an integral C value. It
is a frequent operation but we found it is suspicious to be using almost one
tenth of the total CPU resources. It was indeed not optimally implemented;
Listing~\ref{lst:eval:gprof-pre} and~\ref{lst:eval:gprof-post} shows the
specific code before and after optimization.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language={[ANSI]C},%
  caption={The function \code{bytes2int} before optimization.},%
  label={lst:eval:gprof-pre}]
int64_t value = 0;
int i = size;
while (i) {
    value += ((int64_t) bytes[i - 1]) << (8 * (size - i));
    i--;
}
return value;
\end{lstlisting}

\begin{lstlisting}[language={[ANSI]C},%
  caption={The function \code{bytes2int} after optimization},%
  label={lst:eval:gprof-post}]
switch (size) {
case 1: return bytes[0];
case 2: return bytes[1] | bytes[0] << 8;
case 4: return __builtin_bswap32(*(int32_t*)bytes);
case 8: return __builtin_bswap64(*(int64_t*)bytes);
default: return 0;
}
\end{lstlisting}
\end{minipage}

The optimized version of the function clocked in at the much better 1.4\% CPU
time. Using this technique of measuring, pinpointing, patching and measuring
again we managed to more than triple the speed of some test programs! Currently
the bottleneck of the machine seems to lie somewhere in the stack
implementation, as shown in the following gprof output:

\begin{verbatim}
  %   cumulative   self              self     total
 time   seconds   seconds    calls  ms/call  ms/call  name
 20.24      0.52     0.52 35002981     0.00     0.00  am_exec_instr
  8.17      0.73     0.21 24232831     0.00     0.00  stack_segment_push_element
  6.62      0.90     0.17 35002981     0.00     0.00  am_read_prefixes
  5.64      1.05     0.15 24232831     0.00     0.00  stack_push
  5.06      1.18     0.13 17501491     0.00     0.00  stack_peek
...
\end{verbatim}

Generally it is difficult to determine whether there is a problem with a part of
the code or if it is just a frequent operation in the specific
program. Profiling multiple test programs that utilize different features can
reveal common bottlenecks which should be analyzed further.

Whether there are more fundamental architectural design issues that limits
performance is difficult to say. We have not encountered major issues with how
the modules interoperate.

% JIT

A common form of optimization in modern abstract machines is the process of
compiling a program run-time, rather than prior to execution. As briefly
mentioned in section~\ref{sec:background}, this technique is called just-in-time
compilation, or JIT.~This allows sections of code to be translated to machine
code which the host machine can execute directly. This is done just before the
code is going to be executed, hence the name. The translated machine code can
then be cached and reused. Therefore, if a function is called multiple times,
the machine can save time by paying the overhead of translating it to machine
code, but then saving time every time the same code is to be executed. A
challenge with JITing is balancing the overhead of translating to machine code
versus the saved time during execution. This overhead of just-in-time
compilation is called the ``startup time delay''.

In our earlier performance analysis, we have disabled all forms for such
optimizations to make the execution process of the different machines closer to
that of \thename{}. Figure~\ref{fig:eval:benchmark:jit} shows a benchmark of the
heap workout being run on JVM with and without JIT compilation.

\begin{figure}[H]
  \centering
  \scalebox{0.8}[0.6]{\input{figures/jit}}
  \caption{JIT comparing on JVM}
\label{fig:eval:benchmark:jit}
\end{figure}

The benefit of JIT compilation is striking, it runs more than an order of
magnitude faster. Even with small numbers, we see that the statup time delay
that occurs when using JIT is still outweighed by the faster execution
time. This is obviously a very simple program which means that the JIT process
will be very short, and thus the cost of JITting is most certainly higher for
larger programs.

\subsection{Code Analysis}

Another side of code evaluation is to view the source code from a macro
perspective, including trivial things such as counting source lines of code
(SLOC) and looking at the number of comments per line of
code. Figure~\ref{fig:eval:sloc} shows the division of code in the source
directory and in the tests. The amount of source code is well correlated with
the amount of work performed by each module. The test SLOC are not proportional
to the amount of code in the corresponding module because some algorithms
require much more extensive unit tests than others. For instance the type
conversion algorithms must cover almost all different combinations of types
which results in extensive tests.

\begin{figure}
  \centering
  \input{figures/sloc}
  \caption{SLOC in \thename{}}
\label{fig:eval:sloc}
\end{figure}

The overall architecture has proved itself to be a well suited design that
allowed us to modify functionality, even larger non-trivial changes, without
causing significant trouble or breaking other parts of the system. The division
of modules and their responsibilities was largely based on intuitive decisions
about what categories of functionality were needed. That said, there is room for
improvement in several areas of the system. The instruction execution module has
become large and to some extent unwieldy, and should optimally be split into
smaller sub-modules each implementing a specific family or type of
instructions. There are some repeating patterns of code that we have so far kept
DRY\footnote{``Don't Repeat Yourself''} by use of macros making it difficult to
maintain, reason about and debug and would be better off implemented by
combining smaller and more concise functions.

We have strived to keep the code readable and easy to digest and believe we have
succeeded in this. However, as we have optimized the code, some algorithms have
become more complex and difficult to grasp at first sight, but that is something
we find unavoidable to some extent.

\subsection{Documentation}

The code is fairly well documented using Doxygen, which has been a help when
reviewing and when continuing work on code written by the other. There are
approximately 14 lines of comments for each 100 SLOC which should provide a
decent amount of explanation.

Appendix~\ref{NEEDED} documents how to compile and run the machine on a Unix
system.

\subsection{Future Development}

Our \thename{} implementation lacks several key features that would be required
in a practical system for actual use. Arguably the most important piece missing
at the moment is a working garbage collector. No heap objects are ever freed
which renders the whole system futile for real-world usages. Other shortcomings
include a proper implementation of arrays, an exception handling system, support
for unsafe code execution etc. Further there are many unimplemented variants of
instructions left in the specification, especially in the family of heap object
instructions.

We would need better tooling to aid the development of language front-ends and
for testing the machine continuously. This could potentially be in the form of a
Binutils and GCC port for \thename{} which would provide a full-blown compiler
for the C-family of languages, an assembler and a standard library ``for
free''. The assembler would in itself be a very useful addition to the
development work flow because programs could be hand-written without much effort
and easily tested.

From the benchmarks we can see that performance is not generally anywhere near
the major players on the abstract machine field, which indicates that some heavy
optimizations would be required. The addition of a JIT compiler would alleviate
the performance problems, but is a large, complex piece of machinery.

There is really no end to what features and functionality could be added and
improved, which is true for almost any software project. We do however think
that this is good start and could be the basis for further development.

If we were to do it all again we would most likely chose a language that
provides higher-level primitives to work with, and one that is inherently more
safe in terms of programming errors and memory safety. While C is very fast and
extremely flexible, it does require some effort to keep everything in tune,
which admittedly has been an issue from time to time.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../report"
%%% End:
