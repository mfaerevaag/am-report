\subsection{Paradigm Support}

% good for dynamic
%% composite types are dynamic
Functional and dynamic languages

The groundwork is there, can primitives be used to make support?

\subsection{Performance Analysis}

\subsubsection{Micro Benchmarking}

Heap objects (w/ fields)

Sub-routines

Boxing

General use (stack, algorithms)

Recursion
\begin{figure}[H]
  \centering
  \input{figures/fib}
  \caption{Mean running time of $Fibonacci(n)$, with $n = 0..35$}
\end{figure}

\subsubsection{Comparison with Existing Systems}

``Indentical'' programs in JVM, CIL

Tables, graphs

\subsubsection{Memory Footprint}

\subsubsection{Optimization}

During the majority of the development we did not concern ourselves with
algorithmic optimization, but rather focused on designing suitable data
structures and writing maintainable and concise code that we would be able to
cope with the inevitable changes during the course of the project. As Rob Pike
states it in his first Rule of Programming: ``You can't tell where a program is
going to spend its time.''\cite{pike-rules}. When the machine had reached a
maturity level that assured us that no further major changes to the overarching
design were necessary we started to analyze the code at a much more detailed
level.

Memory leaks are extremely difficult to avoid when writing any non-trivial C
code. One has to meticulously track allocated memory chunks and free them at
just the right time and place to prevent invalid read and writes. The fact that
C largely does not care about what some collection of bytes represents and
allows virtually any casting does not make this easier. One-off errors are very
common when dealing with arrays and pointers which are indeed a central part of
the \thename{} implementation. Luckily there are tools available to aid in
profiling and pinpointing problematic parts of the code. The absolutely
brilliant instrumentation tool Valgrind\footnote{Valgrind:
  \url{http://valgrind.org}} has been an invaluable help in detecting memory
errors and have allowed us to fix, to our knowledge, all problematic leaks.

Valgrind consists of several different tools that analyzes different things such
as threads, stack operations, caches and memory. We have used the memory error
detector called Memcheck. When an executable is run through Valgrind it is able
to track allocations and the corresponding releases made during the entire
execution. If an allocation does not have exactly one corresponding free
operation it is either a leak or a double free (when the same memory region is
attempted to be release more than once).

The output from Memcheck includes a list of errors and a summary of the leaked
amount of memory. When we initially ran Valgrind with the fibonacci test program
(with $n=15$) the leak summary was this:

\begin{verbatim}
==32067== LEAK SUMMARY:
==32067==    definitely lost: 410,033 bytes in 13,187 blocks
==32067==    indirectly lost: 371,808 bytes in 365 blocks
...
==32067== ERROR SUMMARY: 10 errors from 10 contexts (suppressed: 0 from 0)
\end{verbatim}

This says that there are 10 distinct errors causing a total leak of 400kB
memory, which is a very significant amount of memory for such a small and simple
program. Optimally there should be no bytes leaked at all. Valgrind also tells
you exactly where errors are introduced:

\begin{verbatim}
...
==32067== 27,608 bytes in 986 blocks are definitely lost in loss record 4 of 15
==32067==    at 0x4C29F90: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==32067==    by 0x4018F4: stack_create_element (stack.c:128)
...
\end{verbatim}

An allocation is being made in the function \code{stack\_create\_element} and is
not released when the program exits. In this case we were not properly releasing
temporary stack elements. Being aware of the issue we changed the way stack
elements were created and the result was a sum total of 40 bytes leaked all of
which are test data structures that are irrelevant for the actual performance
analysis.

The other form of profiling we did was by use of the equally wonderful tool
called gprof\footnote{GNU gprof:
  \url{https://sourceware.org/binutils/docs/gprof/}} (for GNU profiler). It
works in a somewhat different way than Valgrind: executables are compiled with a
special profiling flag that injects profiling code into the binary. When the
file is executed it generates data that is then run through gprof that performs
the analysis. The output is a detailed overview of the CPU time spent in each
function. It is important to avoid profiling optimized binaries because some
crucial information can be lost during when the compiler performs various code
transformations. For example, if the compiler inlines a function into another,
the former will appear to be using the resources that are actually spent by the
inlined function.

As an example of how we used gprof to pinpoint bottlenecks, an initial analysis
showed that 9\% of the total running time was spent in the function
\code{bytes2int} which converts a sequence of bytes into an integral C value. It
is a frequent operation but we found it is suspicious to be using almost one
tenth of the total CPU resources. It was indeed not optimally
implemented; Listing~\ref{lst:eval:gprof-pre} and~\ref{lst:eval:gprof-post}
shows the specific code before and after optimization.

\begin{lstlisting}[language={[ANSI]C},%
  caption={The function \code{bytes2int} before optimization.},%
  label={lst:eval:gprof-pre}]
int64_t value = 0;
int i = size;
while (i) {
    value += ((int64_t) bytes[i - 1]) << (8 * (size - i));
    i--;
}
return value;
\end{lstlisting}

\begin{lstlisting}[language={[ANSI]C},%
  caption={The function \code{bytes2int} after optimization},%
  label={lst:eval:gprof-post}]
switch (size) {
case 1: return bytes[0];
case 2: return bytes[1] | bytes[0] << 8;
case 4: return __builtin_bswap32(*(int32_t*)bytes);
case 8: return __builtin_bswap64(*(int64_t*)bytes);
default: return 0;
}
\end{lstlisting}

The optimized version of the function clocked in at the much better 1.4\% CPU
time. Using this technique of measuring, pinpointing, patching and measuring
again we managed to more than triple the speed of some test programs! Currently
the bottleneck of the machine seems to lie somewhere in the stack
implementation, as shown in the following gprof output:

\begin{verbatim}
  %   cumulative   self              self     total
 time   seconds   seconds    calls  ms/call  ms/call  name
 20.24      0.52     0.52 35002981     0.00     0.00  am_exec_instr
  8.17      0.73     0.21 24232831     0.00     0.00  stack_segment_push_element
  6.62      0.90     0.17 35002981     0.00     0.00  am_read_prefixes
  5.64      1.05     0.15 24232831     0.00     0.00  stack_push
  5.06      1.18     0.13 17501491     0.00     0.00  stack_peek
...
\end{verbatim}

Generally it is difficult to determine whether there is a problem with a part of
the code or if it is just a frequent operation in the specific
program. Profiling multiple test programs that utilize different features can
reveal common bottlenecks which should be analyzed further.

Whether there are more fundamental architectural design issues that limits
performance is difficult to say. We have not encountered major issues with how
the modules interoperate.

\subsection{Code Analysis}
\subsubsection{Code size (SLOC)}
\subsubsection{Architectural analysis}
\subsubsection{Readability, maintainability}
\subsubsection{Tests, coverage}
\subsubsection{Configurability (macros)}

\subsection{Documentation}

Very brief doxy description

% Usage evaluation
%% Have the problems been solved

% Assembly generation
%% Something about generating code for the machine

% Performance
%% Function calls
%% Analysis

% Future development
%% What's missing
%% What could be different
%% What would next steps be

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../report"
%%% End:
